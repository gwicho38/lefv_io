Hello. Welcome to CS 421. We're going to start a new section of the course with this video. In the first section of the course, we were concerned with writing programs to represent languages and you learned about different kinds of recursion and programming techniques such as continuation passing style, and you learned about algebraic data types and type classes. But now, we're going to talk about how the computer reads these computer programs and turns them into a form it can process. First thing we need to do if we're going to teach a computer to read textual input, process it as a language, is to have a way to specify formally what the language looks like. We do this with the notation called a grammar. So, when you're done with this video, you'll be able to identify the different parts of a grammar. You'll be able identify that terminal and non-terminal symbols, sentences, and productions, and use a grammar to generate something called the parse tree. You'll be able to identify grammars have certain properties such as being left- recursive or ambiguous, and demonstrate why it's important to know such things. Here's the problem we're trying to solve for this part of the course. So, a programmer usually types in a program using a text editor and saves it as ASCII text or perhaps some variant of unicode. Now, this representation is fundamentally a stream of characters. It's difficult to process this directly if we want to treat it as a computer program though. So, by now, you'll have written some interpreters that process abstract syntax trees directly. What we want is a way to convert the text into these abstract syntax trees. In Haskell, descendance from the previous slide might be represented like this. We have some user-defined data type and a separate constructor for each of the language components. The mechanism for converting a block of text into an abstract syntax tree is usually referred to as parsing. Usually, this is broken down into two distinct steps. The first step is to take the incoming stream of characters and break them up into individual words or tokens. The tokens indicate not just the word but the kind of word. For example, the string one, two, three will be converted into an integer token, and the string else will be converted into the else keyword token. This phase is called lexing or scanning. The next phase is called parsing. This is where individual words get combined into sentences. The word parsing can be used in two different ways here. Sometimes, it includes the lexing step and refers to the whole process of converting the text to a tree, and sometimes, it simply refers to the part where we convert the token stream to a tree. Usually, it's clear from context which one we intend them. So, in order to parse the language, we need a formal definition of a language you want to parse. So, like writing programs, it actually has two purposes. On the one hand, we need to be able to communicate to the computer what the grammar is and a formal representation's the only way we can do that. On the other hand, being able to write down the grammar allows us to communicate with each other about it. A good grammar is essential for documenting a language. Grammar is usually assumed that they're operating on tokens, not characters. So, for the rest of the video, we'll assume that that's the case. Now, a grammar has four components. First, it has a set of symbols that represent individual tokens of words. These are called terminal symbols. Second, it has a set of symbols that represent components of trees. These are called non-terminal symbols. The easy way to remember the difference is that when you have a terminal symbol, there are no more work to be done to subdivide it. The trees are like molecules and terminal symbols are like the atoms. Now, these symbols are combined to a set of productions. So, a production is a map from a non-terminal symbol to a string of terminal and non-terminal symbols. Now, finally, we pick one special non-terminal symbol to be the start symbol and this represents the root of the tree we will get when we parse the program. Now, let's look at symbols a bit more closely. As I mentioned in the last slide, a terminal symbol represents something that is atomic such as nouns, verbs, integers, et cetera. It's a lexer's job to put these words together from the individual characters. The non-terminals represent parts of the sentence that could be broken into smaller parts. In English, you might use terms such as prepositional phrase or a dependent clause. Each of these could contain multiple words. Here are a few simple examples describing what sentences look like in English, well, at least, a simplified version of English. Now, think about it, which of these terms would be terminal symbols in the grammar and which would be non-terminal symbols? So, here are the non-terminal symbols; sentence, noun phrase, and prepositional phrase. You can tell they're non-terminals quickly because there's a description of how to assemble them from the component parts. The remaining words; verb, determinant, noun, and preposition are all terminal symbols. Here's one notation we might use to specify a grammar. In this convention, we use a capital letter to represent non-terminal symbols and lowercase letters to represent terminals. So, we have three lines in this grammar. Each of them is called a production. So, a production tells us how to build a symbol. One property of writing a grammar this way. The rules we have written down say that these non-terminal symbols can be produced using the corresponding other symbols on the right-hand side. There are no other rules or indications about when the production is valid. The productions are assumed to be valid all the time. So, this kind of grammar is called a context-free grammar, and it's the most common grammar by far used in programming languages since it's easy to write parsers for them. There are some programming languages that have context-sensitive grammars but they're pretty rare. Here is a parse tree that results from using our grammar on the sentence, "The dog runs under the chair." The S was the start symbol so it's at the root of the tree. Here's another example we might see in the context of a programming language. Here, we have a symbol E that represents expressions and we have four kinds; plus, variables, greater than, and if. If you have an expression, if x greater than y then a plus b else y, we will get a tree that looks like this one. So, now that you've seen what a grammar looks like, let's talk about how you can identify certain properties that a grammar has by inspecting it. Four properties we're going to focus on here are these. You may want to have something called an epsilon production where the tree can be represented by empty string. We have grammars that are right linear where all the productions have a restricted form. We have grammars with recursive productions and sometimes, that recursion occurs on the leftmost part. Finally, we have ambiguous grammars that can produce more than one tree for a given input. So, let's look at these in turn. An epsilon production specifies that a symbol can become nothing or another way of saying this is that a symbol can be considered optional. To do this, we use the Greek letter Epsilon. In our example, all grammar for English sentences, we can use this to add adjectives. So, in English, you can stack adjectives in front of a noun to modify it and you can put as many or as few as you want. To express this with the grammar, we say that the A tree can be an adjective followed by another A tree or else, it could just be an epsilon. It's like saying that the base case is that there are no adjectives at all. Sometimes, a grammar has a form that every production maps a non-terminal symbol to either a terminal followed by another non-terminal, or just a terminal symbol. These grammars correspond to a class of languages called the regular languages. They are important because this is a kind of grammar we usually use to specify lexers. You may have heard of or used regular expressions by now. This is the kind of grammar that describes them. Unlike other examples we use, this kind of grammar works at the character level not just the word level. We're going to have a separate video to describe these. If a production has an instance of itself on the right-hand side, it's called recursive. It turns out that some parsers have trouble if the recursion occurs in the leftmost position. Now, here's a few examples of recursive productions. The if expression has three recursions because an if expression itself is made up of three sub-expression. It's not left recursive however. The next example, E goes to E plus F is left recursive. Usually, the left recursion will be explicit but it's possible to have a kind of mutual left recursion like in this third example. In the context of programming languages though, this is pretty rare. It is very important to be able to identify when a grammar is ambiguous. Some kinds of parsers get stuck if you give them an ambiguous grammar and others simply pick one of the possible trees and discard the others. If that happens, the parser may choose the wrong tree. You'll learn a formal way of detecting ambiguities, but there are two patterns of ambiguity that show up often enough that they're worth learning. If you see either of these patterns in a grammar, then you know the grammar is ambiguous right away. The first is known as the dangling else problem. In some languages, if statements are allowed to omit the else branch. This can cause trouble if you have nested if statements. So, if there's an inner if statement in the then clause of an outer if statement and you see an else and you don't know which if statement the else belongs to. The other form that commonly causes ambiguity is what I call double-ended recursion. It shows up a lot when people try to make grammars have represent arithmetic expressions. To fix the dangling else problem, most languages provide an explicit keyword that represents the end of an if statement like if I or and if. For double-ended recursion, the problem is that the grammar does not specify the precedences are associations of the operations. To fix this, some tools allow you to specify the precedence levels explicitly. You can just tell the tool that plus has lower precedence at times and that you want plus to associate to the right for instance. But you can also encode this information directly into the grammar, and here's how you do it. You create separate non-terminal symbols to represent each level of precedence in the grammar. The lower precedences come first, and the higher precedence levels come later. The way to remember this is that the lower levels of the tree bind more tightly than the upper levels. Therefore, the lower levels represent higher precedence. The other thing you have to do is get rid of the double-ended recursion. The trick is that left recursion represents left associativity and right recursion represents right associativity. You just have to pick which way you want the associativity to go. In the example here, plus expressions are represented by E, and they are right associative. F represents factors which are also right associative, and T represents atomic terms like integers or parenthesized expressions. In the example here, plus expressions are represented by E and are right associative. F represents factors which are also right associative, and T represents atomic terms like integers or parenthesized expressions. So, that's it for this part of the introduction. In the next videos, we'll talk about how to go about using the grammar to parse something. It's a big job, so they'll need to be multiple steps. The first thing we do for many parsers is to determine the first and follow sets of a grammar.
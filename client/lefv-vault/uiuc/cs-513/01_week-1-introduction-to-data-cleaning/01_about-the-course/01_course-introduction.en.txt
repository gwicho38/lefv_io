[MUSIC] Welcome to theory and
practice of data cleaning. In this video, I will give you an overview of the topics
we will be covering in this course. As the course title suggests, we will be
looking at both, theoretical foundations and concepts of data cleaning, but
also learn very practical issues. In particular, you will learn practical skills to
clean data with freely available tools. But first, why do we need to clean data? What is dirty data, anyways? Let us use a simple example
to illustrate the problem. Assume you have identified a data
set that you need to analyze for a data science project. Maybe you have already extracted
the data from various sources and brought it into a tabular format. As you look through the rows
of your data tables, you notice that important columns show
multiple non-unique representations. For example, for dates or locations. Let's take the simplest, yet very widespread issue, different
representations of calendar dates. This comic from xkcd nicely
illustrates the point. It starts with the following,
public service announcement. Our different ways of writing dates as
numbers can lead to online confusion. That's why in 1988, ISO,
the International Organization for Standardization, send a global
standard numeric date format. This is the correct way
to write numeric dates. 2013-02-27. The following formats
are therefore discouraged. Here the comic lists among other quite
confusing but also entertaining variance. The popular US convention of listing the
month, then the day, and then the year. Why is this a problem? Take any data analysis
that you want to do, and that means to aggregate values by date. This is very common. If you have different
representations of dates, your analyzed results will be incorrect. This is what the saying, trash in,
trash out means in this context. So what can we do? Before we can do meaningful data analysis,
we need to do some data cleaning or data wrangling as it is sometimes called. Data wrangling includes steps
such as data extraction, data integration data cleaning proper and
querying. This is something that database experts
have been working on a longtime. A proper databases,
it turns out that these database topics, that is data wrangling and data cleaning,
are usually the most time intensive parts of data science projects, yet they often
don't get the attention they deserve. It seems that here an alternative
80/20 rule applies. The shiny downstream data analysis
is maybe only 20% of the work, but its gets 80% of the PR. In contrast, the all important data
wrangling and data cleaning work only gets 20% of the PR, although it
often at least 80% of the work. We can see that in the early
stages data is gathered, and data quality problems
need to be identified. For example, via form of data profiling. Next, we need to do some data
cleaning proper, for example, in the case of the various date formats,
we need to standardize and normalize the data to make it fit for
our purpose. So what do we mean by data quality here? It turns out that this is not that easy
to define, because someone's sturdy data, or noise, may be somebody else's signal
that they study and need to analyze. This is why the notion of fitness for
use or fitness for purpose is crucial as a guide, so
we can decide what needs to be cleaned and when we can consider data are good or
clean enough for the intended purposes. Now, let me give you
an overview of the topics and tools we are going to
cover in this course. We will start up with a very practical
hands on tools called OpenRefine. OpenRefine was called
Google Refine earlier and is an excellent interactive tool
to profile and normalize data. These do some key deduct
leaning functions. Within OpenRefine, we will sometimes
need to use regular expressions to specify the kinds of data we
want to match and transform. So we will also be learning
about regular expressions soon. After we're done with regular
expressions and OpenRefine, we can then take a data set and
load it into a database system. This allows us to check
the integrity of our data set using a powerful query language for
such tabular data, that is SQL. We'll also learn about a rule based
language called Datalog that allow us to express and
test logic integrity constraints. Using SQL and Datalog, we will then see
how we can check the consistency of our data set, and identify problematic
data that needs further attention. Here you see an example of the kinds of
problems that a data set might still have, after it has been partially
cleaned using OpenRefine. Some of the issues you see here could be
detected and repaired with OpenRefine, but others are clearly better
handled in the database system. For example,
the issues with date of birth or the DOB column is something we can try and
fix with OpenRefine. But what about other
integrity constraints? For example, that every ZIP code in the person table
should also show up in the address table. This can be easily checked with database
technology which is why we will spend some time on learning about integrity
constraint checking in databases. After these very hands-on topics, we will also take a brief look at cutting
edge research approaches to automatically repair databases based on integrity
constraints that need to be satisfied. In the final part of the course, we will
address two related topics, that is, workflow automation and prominence. This is another topic where we'll
get our hands dirty again, that is, we will write some Python scripts
to do simple data cleaning. The focus will then be on,
how we can model such scripts as workflows that document how data was processed as
part of the overall data cleaning phase. This is important to ensure we
produce ability of the old world and data science project. Here, we will also learn more about
the important notion of data providence, which describes the origin and processing history of data as it is
being processed in the workflow. [MUSIC] [SOUND]
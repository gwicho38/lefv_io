Welcome back to theory and practice of data cleaning. In this video, we will ask the question, what is data quality and how we can improve it? So first, let's look again at data quality and data cleaning in context. Data cleaning or data wrangling is a much needed but often under-appreciated phase before data analysis can begin. As we've seen last time, low quality data causes significant costs and that doesn't matter whether we clean the data and just live with the dirty data or whether we spent the effort on cleaning the data. In both cases, it is the low quality data that causes the significant costs. And obviously, we want to apply data cleaning if we have to do it early in the data lifecycle and apply again if we have to, or preferably, we try to create the data without data quality problems. But in the real world, we often have these issues and we need to look at the data errors that are in our data. And also last time we had a brief look at the different types of data errors that a dataset might have. So, we mentioned briefly quantitative errors or outliers, which are focused typically of classes around statistics and the focus in this class qualitative errors, which are syntax or formatting errors, for example, pattern violations and semantic or schema errors such as integrity constraints. Those two kinds will be the focus of this course. Now, a recurring topic here is data quality. And data quality, just like quality in general, is not easily defined as this author Robert Pirsig of a famous novel Zen and The Art of Motorcycle Maintenance says, "Even though quality cannot be defined, you know what it is." So once we see it, maybe we know what it is. We will study some examples as well. However, we want to be a little bit more proactive and we'll look at what data quality is in our context. So first of all, let's make an attempt. And this is a common definition of data quality. Data quality is often defined as fitness for use or fitness for purpose. What does that mean? We can say that data are of high quality if they are fit for use in their uses (by the customers) in operations, decision-making and planning. Another way of saying this or looking at this is to say, they are fit for use when they are free of defects and possess the features needed to complete the operation, make the decision or complete the plan. Or in our case, in data science and data analytics, if they are good enough and free of defects so we can do the data analysis that we want. Again, as usual, I have some references here on the slides. So, this notion of data quality and fitness for use is basically a good one. It comes from the idea that we first have to ask the question, what is it that we want to do with the data? Specifically, what are the questions we're trying to answer from the data using the data? And in this context as we know, that data cleaning of dirty data can be very expensive for creating high quality data in the first place can be very expensive. The question is often, do we even need this particular data set? Do we even need this particular table or column or field in this dataset? And so this is why we ask, is the data maybe already good enough for our use? Can we already answer that question? Or is it hopeless, maybe? There's no way that given certain data set, we can answer the questions that we want. So take for example, you might have a census data set available, you're doing some citizen science project or some data journalism project, and you want to analyze the census data say by region or geographic region or by state or by county. So, some depending on the analysis questions, you may or may not need to know what is the population of a particular area or region or a county. Or if your analysis does not require information about the population, the accuracy and data quality in that column might not be important. Similar for place names, latitude, longitude. So there's a lot of different types of data, different columns in such a data set and clearly depending on the question that you're asking, you may or may not bother to clean up or check the quality of data in certain columns. So, this is where the notion of fitness for use is very helpful. So, but there are cases where this gets tricky. So for example, if you don't yet know what you want to ask of the data, maybe you create data for the first time, the question is, what should I capture? There are so-called minimal information standards that sometimes try to capture the information that you definitely need no matter what the use of the data subsequently. Also, this is challenging the notion of fitness for use, challenging when you are for example, an archivist- a digital archivist, maybe, a digital librarian or somebody who works for a research data library. Their job is to archive the data as it comes to the repository to the archive. But they cannot assume or know anything about the future use of the data, not necessarily. So how would we know whether there's any operations that we need to do? So typically, as an archivist you're not trying to improve the data, that would be in fact, a misuse of your role as an archivist. You're supposed to archive the data as it comes. On the other hand, if you are a data curator, say maybe you work in a Natural History Museum and you're creating a data set there then you might want to improve the data quality because if you have bad quality data, maybe latitude, longitude of specimen data is incorrect or the species names are not standard or dates cannot be recognized and so on. Then the data set that you're maybe maintaining is quite useless for subsequent uses. So in that case, you need to in fact, think about the future use and then possibly need to improve the data. So looking at the broad picture, here are three pillars of data quality that have been identified by a chapter in the Handbook of Data Quality. They are the organizational, architectural and computational pillars. So by organizational pillar, we mean the data quality objectives for the organization. The strategy is to establish roles, processes, policies and standards required to manage and ensure data quality objective. So again, there is implicitly this notion of fitness for purpose and data quality as well. What is our objective? What is it that we want to do with the data? Now, the architectural pillar refers to the technology landscape to deploy a data quality management process or standard or policy. And the focus of this class is really on the computational side, where we look at I.T. tools and computational techniques required to meet data quality objectives. So concrete examples of these are syntax and format normalization, integrity constraints, notions such as provenance or the data lineage and processing history of data and there are many further computational topics, not all of which we are covering in this course. For example, duplicate detection. All right, so what are the common phases and steps in data quality management? So occasionally, we have to actually understand the context of the information or organizational processes. And this is called the context reconstruction. But this phase can sometimes be skipped if this context information and background information is already available from previous analyses. But the next two phases are really critical, pretty much to all data quality management processes. First one then is assessment or measurement, so the idea here is that we measure data quality along relevant dimensions and we assess the data quality by comparing it often, for example, with reference values. And this then allows us to do some diagnosis of the data quality. And often, we can then also find some causes of poor data quality and possibly rule those out in the future or automate the removal of such low quality data. So assessment and measurement is critical. And then, of course, we come to the data cleaning, proper phase, the improvement. So here we are, interested in the selection of the steps and strategies and techniques. Also, the use of tools for reaching the data quality targets. Dimensions of data quality is rather a vast topic. A lot of research has been conducted to look at data quality dimensions in quite some depth, and there are very many dimensions, over 100 or so. However, as you can find from this reading, that is mentioned here, there are a couple of dimensions that have to do with data values that show up pretty much everywhere. And these data quality dimensions, we'll have to take a closer look. They are accuracy, completeness, consistency and timeliness of data. So, what do we mean by accuracy? Accuracy, we mean the extent to which data are correct, reliable and corresponds to the ground truth for the real world values. And often, in practice, we focus on syntax and patterns. For example, very concretely you might use regular expressions to match dates, this is one of our first exercises. So, is it even possible that a certain date is even valid and represents a real world date? Or can we parse a date field in a dataset and it corresponds to another or to all the other date fields in the dataset? Completeness, refers to the degree to which a data set includes necessary information about relevant objects. If a data set is very incomplete, clearly certain uses we cannot allow or will not work. So, sometimes, we have to have mostly complete data or very complete data. What is consistency? Again, here we have actually tools that can check consistency by checking whether integrity constraints are satisfied or violated. So, these are constraints possibly semantic constraints or schema level constrains that we know should hold for a given application for a given dataset or a set of tables, maybe. And so, we look quite closely in the use of database technology to check the consistency of data. And finally, the notion of timeliness, sometimes also called currency, and the related notion of volatility. So, since they change over time, working with the data set might be problematic. And so we have to answer certain questions, if we want to know whether the data is of good enough quality with respect to timeliness. So for example, what is the delay between the change in the real world and the change in the database? If our data is very old, it might be stale, it might be low quality, with respect to timeliness, it might not work for our analysis questions at hand. Another way to ask this is how long is the data a valid in the real world? And more generally, is the data still appropriate for the purpose at hand? To summarize, we have defined data quality as fitness for use. We have learned about three pillars of data quality: organizational, architectural and computational with the latter being the focus of this course. And we've also learned about the quality management phases on text reconstruction and then, the important assessment or measurement of data quality and then improvement of data quality, really the key focus of this course. And last, not least, we have looked at data quality dimensions in particular, the dimentions of accuracy, completeness, consistency and timeliness
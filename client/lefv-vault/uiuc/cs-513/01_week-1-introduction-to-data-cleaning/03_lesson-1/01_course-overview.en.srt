1
00:00:07,099 --> 00:00:10,809
Welcome to theory and practice of data cleaning.

2
00:00:10,809 --> 00:00:15,230
In this video, I will give you an overview and introduction of the course.

3
00:00:15,230 --> 00:00:18,920
Let's first have a look at data wrangling versus data analytics.

4
00:00:18,920 --> 00:00:21,754
Data wrangling refers to data processing

5
00:00:21,754 --> 00:00:25,205
that allows meaningful analysis to begin subsequently.

6
00:00:25,205 --> 00:00:27,515
It includes steps, for example,

7
00:00:27,515 --> 00:00:29,885
data extraction, transformation, and loading,

8
00:00:29,885 --> 00:00:33,429
also known as ETL, but also data integration,

9
00:00:33,429 --> 00:00:36,329
data cleaning, querying, and repairing databases.

10
00:00:36,329 --> 00:00:40,115
Unfortunately, database people are often not very good at PR,

11
00:00:40,115 --> 00:00:44,329
so most of the glamorous data analytics gets all the attention and

12
00:00:44,329 --> 00:00:47,509
the equally important and often much more time consuming task

13
00:00:47,509 --> 00:00:49,340
of data wrangling and data cleaning.

14
00:00:49,340 --> 00:00:51,954
If you look at what skills data scientists should have,

15
00:00:51,954 --> 00:00:54,215
we can look for example, as a study here,

16
00:00:54,215 --> 00:00:57,619
from CrowdFlower.com where the company has extracted

17
00:00:57,619 --> 00:01:01,610
LinkedIn profiles relating to data science job openings.

18
00:01:01,610 --> 00:01:06,525
And you will notice that database skills are rather dominantly required here.

19
00:01:06,525 --> 00:01:12,185
So for example, you see that SQL is the most in-demand skill from this particular study,

20
00:01:12,185 --> 00:01:16,549
but also there are indirectly other database skills required, for example,

21
00:01:16,549 --> 00:01:19,469
for NoSQL databases, but also for Oracle,

22
00:01:19,469 --> 00:01:21,709
MySQL, Postgres and so on.

23
00:01:21,709 --> 00:01:24,079
Now, in this course, the focus is not on teaching you

24
00:01:24,079 --> 00:01:29,045
database skills although it will be helpful if you have such skills already.

25
00:01:29,045 --> 00:01:33,989
On the other hand, we will employ ideas from databases and, in particular,

26
00:01:33,989 --> 00:01:37,409
would like to teach you how to think like

27
00:01:37,409 --> 00:01:41,569
a database person because this will become very handy in our tasks.

28
00:01:41,569 --> 00:01:45,900
Now first let's take a look at what the costs are that result from low-quality data.

29
00:01:45,900 --> 00:01:48,930
Throughout this course, I will give you examples of

30
00:01:48,930 --> 00:01:52,939
readings that you can read accompanying the course material.

31
00:01:52,939 --> 00:01:57,210
And here's an example of such a reading from the "Handbook of Data Quality."

32
00:01:57,210 --> 00:02:00,670
It's called "Cost and Value Management for Data Quality."

33
00:02:00,670 --> 00:02:06,899
This chapter reports on many of the costs that can occur as a result of low data quality.

34
00:02:06,899 --> 00:02:10,055
And often these costs are reported to be in the billions of dollars,

35
00:02:10,055 --> 00:02:11,819
but they can also occur sort of in

36
00:02:11,819 --> 00:02:15,389
more modest forms for the long-tail of science, that is,

37
00:02:15,389 --> 00:02:19,439
where scientists who were just doing their lonely job of data analysis

38
00:02:19,439 --> 00:02:24,030
often find it very difficult to create data or collect data of high quality.

39
00:02:24,030 --> 00:02:26,474
Also in data journalism for example,

40
00:02:26,474 --> 00:02:32,814
when people want to collect open-source data and then analyze it and write about it.

41
00:02:32,814 --> 00:02:37,139
So, low-quality data is a issue in big industry but also in academia.

42
00:02:37,139 --> 00:02:39,360
So, it's a big data science issue.

43
00:02:39,360 --> 00:02:40,914
From the same article,

44
00:02:40,914 --> 00:02:47,044
you can also have an overview of what kind of costs exists due to data quality issues.

45
00:02:47,044 --> 00:02:48,479
For example, in the one hand,

46
00:02:48,479 --> 00:02:51,504
you can have the costs that result from low data quality,

47
00:02:51,504 --> 00:02:53,664
so the effects of low data quality.

48
00:02:53,664 --> 00:02:55,669
So there are direct and indirect costs.

49
00:02:55,669 --> 00:02:57,719
On the other hand, you can try to, of course,

50
00:02:57,719 --> 00:03:00,840
look at what happens to improve the quality of data.

51
00:03:00,840 --> 00:03:02,778
What are the costs there, for maybe,

52
00:03:02,778 --> 00:03:05,430
preventing the dirty or low-quality data in

53
00:03:05,430 --> 00:03:08,280
the first place or detecting the data and then,

54
00:03:08,280 --> 00:03:10,094
of course, for repairing it.

55
00:03:10,094 --> 00:03:16,360
Again, I've given you here an example reading to look further into these issues.

56
00:03:16,360 --> 00:03:20,544
But to get a feeling of what these issues are, to get more specific,

57
00:03:20,544 --> 00:03:22,699
let me just enumerate a few of them.

58
00:03:22,699 --> 00:03:28,020
So for example there are data maintenance costs or personal costs, data search costs.

59
00:03:28,020 --> 00:03:31,655
All of these costs can be increased due to low-quality data.

60
00:03:31,655 --> 00:03:34,620
Semantic confusion cost, if you're looking for the wrong data,

61
00:03:34,620 --> 00:03:36,224
if you have to re-enter data,

62
00:03:36,224 --> 00:03:38,025
if you misinterpret data,

63
00:03:38,025 --> 00:03:40,694
you might lose revenue if in industry or you might

64
00:03:40,694 --> 00:03:44,129
lose current or future customers, or even orders.

65
00:03:44,129 --> 00:03:49,289
So there's a very large number of effects that result from low-quality data.

66
00:03:49,289 --> 00:03:52,860
Now let's turn to the big idea of data cleaning and what we're doing in this course.

67
00:03:52,860 --> 00:03:54,405
So the overall idea is to,

68
00:03:54,405 --> 00:03:55,835
what we're aiming to understand,

69
00:03:55,835 --> 00:03:58,530
to assess, and then improve data quality.

70
00:03:58,530 --> 00:04:01,465
So you can remember these issues with the following three Qs.

71
00:04:01,465 --> 00:04:03,289
We have quality dimensions.

72
00:04:03,289 --> 00:04:08,076
That means data should be of high quality according to a number of dimensions;

73
00:04:08,076 --> 00:04:09,270
it should be accurate;

74
00:04:09,270 --> 00:04:10,435
it should be timely;

75
00:04:10,435 --> 00:04:13,050
it should be relevant, complete and so on.

76
00:04:13,050 --> 00:04:14,935
But then, data quality,

77
00:04:14,935 --> 00:04:16,904
how is it defined in the first place?

78
00:04:16,904 --> 00:04:19,375
We can think of it, a state of high quality,

79
00:04:19,375 --> 00:04:22,889
if it allows us to answer the questions that we want to ask.

80
00:04:22,889 --> 00:04:24,625
So this term is also sometimes used,

81
00:04:24,625 --> 00:04:27,394
it's called fitness for use or fitness for purpose.

82
00:04:27,394 --> 00:04:31,194
Is the quality of the data sufficient to answer the questions that we have?

83
00:04:31,194 --> 00:04:35,490
And then finally, we turn the questions often into queries,

84
00:04:35,490 --> 00:04:38,839
and this is where we take a database person's perspective.

85
00:04:38,839 --> 00:04:41,324
So for example, we want to execute

86
00:04:41,324 --> 00:04:45,675
database queries in order to get an overview of what's kind of data we have,

87
00:04:45,675 --> 00:04:47,189
that's called data profiling.

88
00:04:47,189 --> 00:04:50,100
Or, more specifically related to data cleaning,

89
00:04:50,100 --> 00:04:52,284
we might want to check the integrity constraints.

90
00:04:52,284 --> 00:04:56,795
So, certain quality metrics can be translated into integrity constraints,

91
00:04:56,795 --> 00:05:01,230
and we can then check whether data satisfies these integrity constraints and,

92
00:05:01,230 --> 00:05:03,209
if necessary, deal with the outliers,

93
00:05:03,209 --> 00:05:05,790
deal with exceptions and so on, and repair the data.

94
00:05:05,790 --> 00:05:08,055
So let's also look at data cleaning in context.

95
00:05:08,055 --> 00:05:11,490
Where does it fit in the larger data lifecycle?

96
00:05:11,490 --> 00:05:14,160
So, initially, data comes from various sources,

97
00:05:14,160 --> 00:05:17,420
and so we have to gather and select that data that we want to work with.

98
00:05:17,420 --> 00:05:19,495
We then have to profile the data.

99
00:05:19,495 --> 00:05:24,095
We want to identify and then quantify that data quality issues that the data might have.

100
00:05:24,095 --> 00:05:26,040
And then, depending on the error types,

101
00:05:26,040 --> 00:05:29,040
we can deal with the causes and try to repair the data.

102
00:05:29,040 --> 00:05:30,680
In the data cleaning phase,

103
00:05:30,680 --> 00:05:32,074
the proper data cleaning phase then,

104
00:05:32,074 --> 00:05:35,954
one of the main task we often do is we standardize the data,

105
00:05:35,954 --> 00:05:37,394
we bring it into a common format,

106
00:05:37,394 --> 00:05:42,329
we normalize it, and we might even be using controlled vocabularies so that

107
00:05:42,329 --> 00:05:47,310
the values that we find in the data conform to these standard vocabularies.

108
00:05:47,310 --> 00:05:52,194
And then finally, we have the data analysis phase proper.

109
00:05:52,194 --> 00:05:57,779
This is, of course, covered in other courses when we do data analysis methods.

110
00:05:57,779 --> 00:06:01,350
This is not always such a linear pipeline, the data lifecycle,

111
00:06:01,350 --> 00:06:07,361
but rather we can even use sometimes methods from data analysis for data cleaning,

112
00:06:07,361 --> 00:06:08,730
so it is often a cycle.

113
00:06:08,730 --> 00:06:10,589
I want to also mention that data integration and

114
00:06:10,589 --> 00:06:13,139
data warehousing are issues where data quality is

115
00:06:13,139 --> 00:06:17,639
particularly important because we combine information from various sources.

116
00:06:17,639 --> 00:06:21,074
And because of the various sources where we integrate the data from, we,

117
00:06:21,074 --> 00:06:25,500
in particular, find data quality issues often that we need to deal with.

118
00:06:25,500 --> 00:06:28,110
So here is a simple taxonomy of error types.

119
00:06:28,110 --> 00:06:32,350
This is taken from one of the readings associated with this course.

120
00:06:32,350 --> 00:06:35,670
It's called "Detecting Data Errors: Where Are We and What Needs to Be Done?".

121
00:06:35,670 --> 00:06:36,879
So at the very highest level,

122
00:06:36,879 --> 00:06:40,824
we can think of quantitative errors versus qualitative errors.

123
00:06:40,824 --> 00:06:44,230
For quantitative errors, we think of them as outliers, for example.

124
00:06:44,230 --> 00:06:47,060
That means if certain values of the data deviate

125
00:06:47,060 --> 00:06:50,274
significantly from the distribution of the values.

126
00:06:50,274 --> 00:06:53,319
To detect outliers, we typically apply methods from statistics,

127
00:06:53,319 --> 00:06:54,836
data mining, and machine learning,

128
00:06:54,836 --> 00:06:57,129
and these are outside the scope of this course.

129
00:06:57,129 --> 00:07:01,149
On the other hand, qualitative errors often deal with syntactic variations,

130
00:07:01,149 --> 00:07:04,899
maybe of different ways to spell a place, a location,

131
00:07:04,899 --> 00:07:09,579
different formats to spell dates for example,

132
00:07:09,579 --> 00:07:14,704
and these syntactic violations we need to deal with and this is part of this course.

133
00:07:14,704 --> 00:07:16,345
Similarly, at a higher level,

134
00:07:16,345 --> 00:07:20,495
we have then also schema issues and integrity constraint violations.

135
00:07:20,495 --> 00:07:23,560
So for example, people who know databases will know there

136
00:07:23,560 --> 00:07:27,310
are integrity constraints such as functional dependencies,

137
00:07:27,310 --> 00:07:30,670
foreign key dependencies or inclusion dependencies.

138
00:07:30,670 --> 00:07:34,389
These allow us to check the quality of the data with respect to

139
00:07:34,389 --> 00:07:39,279
certain constraints that come from the schema or from the semantics of an application.

140
00:07:39,279 --> 00:07:43,295
And so syntactic and schema level issues are the focus of this course.

141
00:07:43,295 --> 00:07:46,629
There are also other issues that can be considered qualitative errors,

142
00:07:46,629 --> 00:07:50,819
for example, detecting duplicates and removing duplicates.

143
00:07:50,819 --> 00:07:53,514
So let's take a closer look at the course themes,

144
00:07:53,514 --> 00:07:56,920
topics, and in even the tools that we're going to study in this course.

145
00:07:56,920 --> 00:07:58,839
So among the syntactic issues,

146
00:07:58,839 --> 00:08:01,240
we will begin with studying regular expressions,

147
00:08:01,240 --> 00:08:03,970
those defined patterns that can be used to match,

148
00:08:03,970 --> 00:08:05,995
extract, and transform data.

149
00:08:05,995 --> 00:08:09,175
That is, we will deal with syntactic variations in the data.

150
00:08:09,175 --> 00:08:11,225
A particular tool that we're going to use,

151
00:08:11,225 --> 00:08:14,060
and it's rather powerful, it's called OpenRefine.

152
00:08:14,060 --> 00:08:15,468
This is an open source tool,

153
00:08:15,468 --> 00:08:17,740
one of the few open-source tools for data wrangling.

154
00:08:17,740 --> 00:08:20,339
In order to deal with schema and semantic issues,

155
00:08:20,339 --> 00:08:22,410
we'll wear a database person's hat.

156
00:08:22,410 --> 00:08:26,095
We're gonna use database technologies for data profiling, for querying data,

157
00:08:26,095 --> 00:08:28,435
but also for checking integrity constraints,

158
00:08:28,435 --> 00:08:31,240
and even we are venturing a little bit into database repair.

159
00:08:31,240 --> 00:08:35,654
The languages and tools we're gonna use are Datalog and SQL.

160
00:08:35,654 --> 00:08:40,029
And finally, in the last block of topics and themes here,

161
00:08:40,029 --> 00:08:42,552
in this course, we look at synthesis issues.

162
00:08:42,552 --> 00:08:47,799
That means when we put together a data analysis pipeline and the data cleaning pipeline,

163
00:08:47,799 --> 00:08:49,855
for example, using scripts.

164
00:08:49,855 --> 00:08:53,575
In this context, it will be important to get an understanding of

165
00:08:53,575 --> 00:08:57,235
what is the workflow that puts everything together.

166
00:08:57,235 --> 00:09:00,370
So we will look at workflow automation and in this context in

167
00:09:00,370 --> 00:09:04,659
particular it's important to get also an understanding of the provenance of data.

168
00:09:04,659 --> 00:09:08,200
Provenance describes the lineage and history, processing history,

169
00:09:08,200 --> 00:09:12,265
of data, and this is something we will study towards the end of this course.

170
00:09:12,265 --> 00:09:14,500
And the tool we're gonna use there is

171
00:09:14,500 --> 00:09:18,804
a prototypical tool that's developed as part of a research project,

172
00:09:18,804 --> 00:09:20,034
and it's called YesWorkflow.

173
00:09:20,034 --> 00:09:24,589
It allows to put to to model scripts that automate

174
00:09:24,589 --> 00:09:30,325
data cleaning or data analysis tasks as workflows and then work with this workflow,

175
00:09:30,325 --> 00:09:34,500
for example, to understand data dependencies or to query provenance information.

176
00:09:34,500 --> 00:09:37,350
So let's look take a quick look at some of these tools.

177
00:09:37,350 --> 00:09:39,200
So here's the OpenRefine tool.

178
00:09:39,200 --> 00:09:42,235
OpenRefine was formerly called Google Refine,

179
00:09:42,235 --> 00:09:44,014
and it's now an open-source tool.

180
00:09:44,014 --> 00:09:47,169
So we will explore data using this tool.

181
00:09:47,169 --> 00:09:50,361
It has a feel quite similar to a spreadsheet,

182
00:09:50,361 --> 00:09:51,370
so when you load the data,

183
00:09:51,370 --> 00:09:54,039
it looks like you're working with a spreadsheet application,

184
00:09:54,039 --> 00:09:58,959
however there's particular functionality for data cleaning built into OpenRefine.

185
00:09:58,959 --> 00:10:02,409
Once we have put into OpenRefine and cleaned it,

186
00:10:02,409 --> 00:10:04,090
we can load it into a database.

187
00:10:04,090 --> 00:10:05,815
Once in a database, however,

188
00:10:05,815 --> 00:10:08,306
not all the data issues have gone away,

189
00:10:08,306 --> 00:10:11,169
and so instead we need to work with

190
00:10:11,169 --> 00:10:15,845
database technology to identify and possibly repair data problems.

191
00:10:15,845 --> 00:10:17,514
So here on this screen,

192
00:10:17,514 --> 00:10:21,100
you'll see two tables: a person table and an address table.

193
00:10:21,100 --> 00:10:24,445
And you will see a number of data quality issues here.

194
00:10:24,445 --> 00:10:26,345
So let's quickly go through them.

195
00:10:26,345 --> 00:10:32,059
So, for example, you see that the identifier of a person should probably be unique,

196
00:10:32,059 --> 00:10:37,355
but on the left, you see that 43 is used as an identifier for two different rows.

197
00:10:37,355 --> 00:10:39,820
So that's clearly a data quality problem.

198
00:10:39,820 --> 00:10:43,809
Also, the format of names seems to be not unique, so we have,

199
00:10:43,809 --> 00:10:45,995
for example, last name comma first name

200
00:10:45,995 --> 00:10:48,855
or the first name followed by last name without a comma.

201
00:10:48,855 --> 00:10:50,889
The format of the date of birth column -- that's

202
00:10:50,889 --> 00:10:54,294
the third column in the person table -- is vary is varying.

203
00:10:54,294 --> 00:10:55,924
There's very different formats.

204
00:10:55,924 --> 00:11:00,365
So this will be very problematic when we want to query this data or analyze this data.

205
00:11:00,365 --> 00:11:04,940
So, ideally, this should have been taken care of before we load the data into a database.

206
00:11:04,940 --> 00:11:06,700
So there are numerous issues here with

207
00:11:06,700 --> 00:11:09,759
this data due to different representations and formats.

208
00:11:09,759 --> 00:11:12,355
There are literally contradictions in the data,

209
00:11:12,355 --> 00:11:15,304
so you can find here, analyzing the tables,

210
00:11:15,304 --> 00:11:17,289
you can try and find these contradictions.

211
00:11:17,289 --> 00:11:19,629
There are incorrect values, typos.

212
00:11:19,629 --> 00:11:24,610
There's duplicates, and there are also integrity constraints violated that, for example,

213
00:11:24,610 --> 00:11:29,774
that database system normally would detect or enforce.

214
00:11:29,774 --> 00:11:33,230
Last not least, we might also have issues where the data is incomplete.

215
00:11:33,230 --> 00:11:35,600
So this is indicated in the phone column, for example,

216
00:11:35,600 --> 00:11:39,320
there are two rows where the phone numbers apparently are not given.

217
00:11:39,320 --> 00:11:42,554
One is with a special value, lots of nines there,

218
00:11:42,554 --> 00:11:45,284
and the other is an explicit null value.

219
00:11:45,284 --> 00:11:48,409
All of these issues are data quality issues that we might

220
00:11:48,409 --> 00:11:51,735
encounter even after having used the tools such as OpenRefine,

221
00:11:51,735 --> 00:11:55,149
and we will study database means to deal with these issues.

222
00:11:55,149 --> 00:11:56,700
So in the last part of this course,

223
00:11:56,700 --> 00:11:58,669
we will cover workflows and provenance.

224
00:11:58,669 --> 00:12:01,159
This is one of the aspects mentioned before the synthesis,

225
00:12:01,159 --> 00:12:06,840
when we have workflow automation to automate data cleaning or data analysis pipelines.

226
00:12:06,840 --> 00:12:08,524
In practice, often scripts are used,

227
00:12:08,524 --> 00:12:10,909
and we will study a particular tool called

228
00:12:10,909 --> 00:12:14,870
YesWorkflow that allows to put annotations inside of scripts,

229
00:12:14,870 --> 00:12:17,635
thereby revealing and recreating a workflow.

230
00:12:17,635 --> 00:12:23,779
We can then use this workflow diagram and query it and understand data dependencies,

231
00:12:23,779 --> 00:12:26,330
for example, and the data lineage and provenance of

232
00:12:26,330 --> 00:12:30,450
data coming out of such workflows or scripts.
[MUSIC] So we're going to start a new topic called
Workflow Automation and Provenance. So before I go into the details,
let me just introduce the problem. There is something that has been
called a reproducibility, crisis and it's hitting the airwaves. So I've seen this, for
example, in certain magazines, you see this in Science and the Economist,
in Nature or Scientific American. So I've put some covers from journals, like How Science Goes Wrong,
very dramatic. Or not too long ago I heard on BBC4,
I was in the UK at the time. I heard just in the morning radio a report
about this reproducibility crisis. And what was mentioned is, so the fundamental problem is that a lot of
published studies are hard to reproduce. And sometimes what is the reason,
what might be the reason? Maybe they're hard to reproduce
because they're wrong, yeah? That's why you can't reproduce them,
they're just not true. Another reason why something might
be hard to reproduce is, of course, if you haven't documented well enough. So it could be reproducible,
if only it had more details. Because you have not done a not so
great job at providing the details. That is what makes it basically
hard to reproduce or impossible. And there are different,
there are different causes for this. For example,
the culture in science, in general, rewards novel, eye-catching results. Actually, it's also more
of a societal problem. Everybody wants to report the greatest
findings, and some boring details or non-findings are often
not really published. So the resources that went into this
BBC report that I can link to as well. Are for example, you actually make
mistakes in your mathematics, or in your statistics. The selective reporting, you only report
certain things and not other things. And then generally, the desire to hype
up things can lead to these aspects. Now in particular, the thing we want to focus on in this
course is computational reproducibility. Okay, so different sciences have
different reproducibility crisis, but our focus is really
computational reproducibility. So when you create a scientific workflow,
for example for data analysis pipeline, you might use
a particular workflow system for that. Or you might do it sort of the old
fashioned way, and a very common way and in fact very powerful way. So not necessarily old fashioned, it might also be practical
to do this using scripts. You might use R which is an open
source statistics platform and system, MATLAB, Python and so on. In order to facilitate this
reproducibility for computational and data scientists in their studies,
often what you want to do is to automate. What you do, we'll automate it so it's not a one of a kind thing that
you yourself forget how you did it. That's why you want to use
workflow systems and scripts. You want to be very transparent about it,
so you want to document thinks, keep log files around,
note files around, a digital notebook. And then want to to keep
something called provenance, which is the processing history and
lineage. Some of this can be automated, so that the tools themselves somehow
trace what it is that they're doing. Before I go into these workflows and
provenance in more detail, I just want to point out something
about reproducibility studies. Because nowadays it's actually
become more fashionable and more feasible to actually study,
not something just brand new. But really just reproduce something and
create a publication out of that. So these reproducibility
studies themselves, traditionally they're not super exciting,
often. But nowadays there's a recognition
that they are in fact very valuable. And they need to be more supported and
more actively pursued. So from the reproducibility studies here,
the question's often also, what is the outcome of such a study? If you think about it, a successful
outcome of a reproducability study is at first, of course, a good thing. It increases the trust in a prior study, because you were able
to reproduce something. Means well, there's a good chance that what the other
guys reported is not a fluke, it's real. On the other hand, unless the previous
study was just outrageous. If it was just a good, solid study,
that's not a huge surprise. And the information gain,
the gain in terms of knowledge and information is actually not that high. Again, if it was an outrageous claim,
you confirm it, that's great. But if it was sort of something to
be expected, and now you redid it, and, again, it is confirmed,
that's slightly less exciting. On the other hand, if you have
a failed reproducibility study, there's actually a lot
you can learn from that. So on the one hand, of course,
you decrease the trust in the prior study. Maybe you falsify it even, and
you say this prior study is just wrong. Because we tried it, we followed exactly
the procedures and look what we got. But it's certainly surprising,
because you do normally expect the reproducibility study
to yield the same result. So the failure yields also new
information, new knowledge. You'll learn also probably something
about the flaws maybe that the original study had. So learning from failures is something
that applies generally in life. This is how we learn, we try out things,
we fail, and then we know better. Okay so the idea is that in many
of these computational studies, we want to also facilitate
the way that errors are detected early and often in a way. This is actually also very related to
how you should, to programming, right? You want to in some sense
write programs in such a way that they don't silently fail and
just move on. You want,
if things go in a way that they shouldn't. You want the system to raise an exception
or stop or do something about it. Okay, so much for
the reproducibility studies. But now let's turn to the topic of
workflows which deal with automating computational tasks. Could also be automating business tasks,
they're called business workflows. But our focus is on sort of these signs,
data signs, workflows, scientific workflows, as they're called. So I like to use this acronym, ASAP, as a mechanism to remember. As a mnemonic to remember what some of the
goals are of scientific workflow systems. First and foremost,
it's all about automation, right? If you want to automate
a particular data analysis job, or data cleaning job,
it's good to have a script. So if you compare back,
if you look at OpenRefine, in OpenRefine you will
interactively work with the tool. But maybe you can extract some
recipes that you can then apply on similar datasets over and
over again. So maybe you've once cleaned up
cleaned up certain names and map them to standard names. Wouldn't it be nice if that
recipe could be reused later? So you put it maybe into a script, yeah. Scaling is another advantage that
workflow systems try to address. Things that work on your laptop,
with a small dataset, a couple of thousand rows
might not naturally scale up, for use cases where you have a million
rows, or many millions or more. And so, sometimes workflow systems provide
access to parallel computer resources. They have libraries, maybe you can
work with a two cluster, for example, in the background. And do that fairly,
transparently maybe, that's the idea. There's some systems and approaches, for
example Swift here is a workflow language. It's a little bit more like a scripting
language, but that has support for certain parallel operations. So which then would make it easier
to scale up certain analysis. So that's another thing, what you
want to do with the workflows approach. Then the next letter here is abstraction. Abstraction is when you share your
work with somebody else, you can say, here's my code. Look, I'm a good citizen,
I put it in GitHub, take it. Well, if you've ever seen somebody else's
code that's complex and so-so documented. It doesn't mean that you
really know what's going on. And if, you know,
you have to make a decision. Do I invest, now, a couple of days
af my life trying to work with this? Or is this even not doing the right
thing that I want to use? So wouldn't it be nice if you had a higher
level description that really shows you what this workflow is doing? Yeah, and so workflow systems
themselves often have a visual layout. A sort of diagramming layout,
looks a little bit like a flowchart. But technically it's often
a data flow diagram, which is basically showing you the steps
and how the data flows between the steps. If you have such a thing, that's helpful. To share what it is that you're doing with
others who then can decide whether they want to adopt or not adopt this workflow. Also, sometimes,
the systems are fairly good at allowing you to easily change
the workflow diagrams. By taking out components,
plugging in new components and so on. And then last, not least,
this notion of provenance. As I mentioned briefly, it is the idea
that the processing history and the lineage of data,
how data was derived from other data. It would be nice if that could
be automatically tracked and captured, and also then queried. So you understand, if you have
questions about your data history of the family tree of
your data if you like. Wouldn't it be nice if you
could ask questions about that? And this in particular leads to
sort of a traceable data history. And this also facilitates in that
sense reproducible science if you can sort of debug and
result after the fact. You've created a data product, and later you'll learn that some
of your input data was faulty. There was some quality issue or some
calibration wasn't right, or something. You could, and maybe you're sitting on all
of these datasets that you've derived. And among the input data was one
dataset that was problematic. Now it wouldn't be nice if
I could tell which of my products are sort of contaminated or
tainted by this. So if you kept carefully track
of your derivation history, you could answer that question. And again the idea is that
the workflow system, actually, hopefully should support that. Because you're working in
a controlled environment, you're working with the system. So hopefully that system
can tell you that. As I will say in a little moment, a lot of people prefer to work
with the environments they know. Scripting environments, R, Python for
example, and then, the task shifts now. You ask now, okay, well R and Python natively might not have this
support of keeping track off data lineage. So maybe you can add new libraries,
new components, well then take that job and support that
reproduceability through prominence track. All right, so so much about the workflows,
this is a very high level view, yeah, and I have just these four letters there,
ASAP, there's more to be said. Here's an alternative view,
I like to think of this as the ten commandments from my
colleague and collaborator Tim McPhllips. So, there's more details,
I don't want to go into all of them, just let me maybe highlight a few of them. Again, this is basically saying
what should a workflow system provide in terms of functionality. You see the automation is there, excuse
me, you see the scheduling is there for parallel resources. Managing the data flow, of course, enabling scientist to
change to workflows easily. and then there's these two notions and
we're going to hear more about these two notions of prospective and
retrospective provenance. So provenance is normally
about history about the past. What's the origin of something? How did this something get here? Yeah? So the term retrospective provenance
sounds like okay well that's obvious provenance is obviously retrospective. But some people have introduced
this term prospective provenance which now forces us to
sometimes distinguish the two. So retrospective provenance is our
typical provenance, what has happened. But prospective provenance is
a very powerful construct as well. It's basically about predicting what
a workflow will do when you execute it. So basically when you put a prospective
provenance graph out there, you can call a prospective provenance
actually the workflow graph. It's telling you what the dependencies
will be if you run it. And then you can sometimes you can
actually do very interesting things. This is is kind of
subject of some research, how can you reconcile prospective and
retrospective?. So i work for diagram is a form
respective permanence that tells how a certain data flow works. What steps are executed, and
in what order and what depends on what. When you now run the extra
workflow on the script and then capture retrospective provenance
now you say well, does this align, does this match actually what
the workflow diagram was claiming. So is the predicted prospective provenance
compatible with The actual observed and recorded problems. So that a rather interesting topic. So revealing inquiring provenance
is another important aspect and we will actually end this
classwork with that as well. Okay, so this is sort of a more detailed view of
what workflow systems should be doing. [MUSIC] [SOUND]
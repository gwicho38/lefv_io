Sometimes, when you make an alignment like this one here, it's not obvious at all but it is the case that it is logically inconsistent. You cannot have that. It's like saying X is greater than Y or that Y is also greater than X. Well, that just does not work. You can't have that. We can't say that two sets overlap and they're also disjointed. It just doesn't work. But if you put this in an input alignment, it's not obvious that it's inconsistent. So what you need to do that is to debug or repair your alignment. And what we create there, so we've developed an approach or adopted an approach that's called model-based diagnosis. Here's an example. There's four input articulation, so the expert has made four connections that he or she thinks reflect reality, and those sort of four connection, there's two less than or three less than and one equals. And together with the taxonomies, you can show that that leads to a logical contradiction. These four articulations, these four input articulations, A1 through A4, taken together are logically inconsistent. So what the system can do, the tool can do is, OK, let's find, let's look at all the subsets, maybe some of them at least work. And what this is saying is that A1, A2, A4 and A1, A3, A4 and A2, A3, A4 – those combinations actually do work. But putting A1, A2, and A3 together already, only three out of the four already create trouble. And, I highlighted these particular – the green and the red one, as the green ones are called the maximum consistent subsets of the ones that you want to have, and the red are the minimal inconsistent. So you can either try to start from the green end and then just add something to it. That still works. You know that's something that you're trying to add to it, it can't be an A3 for this combination; it cannot be an A2 for this combination; and it cannot be an A1. It must be another A1, so to say, that you have to plug in. Or you can look at these three and say, "Well, why are these inconsistent anyway?" So there's something, maybe my A1, A2, and A3 don't quite work together. If you then pick something that is consistent, then you might drop off the other end of the pool or the other end of the cliff. You say now that I've repaired it, maybe you look at this world: A2, A3, A4. There's still something missing, that A1 still hasn't been quite prepared to. So now you might have an ambiguous situation. Given the input, if you drop one of the constraints, let's say A1 – I don't know which one of these is A1, but one of these four links is A1. If you drop that, now you get different solutions. You say, well maybe B is below E or E is below B. OK, well, that's a difference. F is below C or C is below F. Or maybe this is how they look: B, C, and A in one taxonomy are related to the other taxo-. So, the yellow team, the yellow taxonomy, is kind of a hat on top of the green taxonomy, or they're more into leaf like this. We don't know this. So, in general, what the tool is doing – we start here at the top – this is kind of the workflow of the taxonomy alignment problem. We start with two taxonomies in the set of articulations, the A's. This is modeled in logic. Then we start up, and now here's a little bit of an AI system if you like. That's a logic-based system. We have different reasoners here at our disposal. And once we do this, we get either a unique solution, in which case we can stop and declare victory; or, we get an inconsistent solution, in which case we have to do this diagnosis and look at the maximal consistent subsets, try to extend those or we repair the minimal inconsistent ones, and then we go back and try again. So that's the inconsistent case. Or if it's ambiguous, then we get lots of possible worlds. We can say, "Well, it's ambiguous. We're done." Or you try to pick one of the possible worlds and argue for it, and then say, "Well, this is the possible world that I think that reflects reality." And then you can extract a smallest subset of input articulations that gives you that world. It feels like a little bit cheating because you say. "OK, your input was ambiguous." Here's like a bouquet. Here's a set of options, and then you pick one set. This is how it should be. But the tool will then allow you to say, "OK, if this is what you want. Here are the inputs that you should have assumed to get that." And then you can argue about the validity of those input alignments. So here's some more realistic examples. So for example, this is a particular Weevil classification, actually two Weevil classifications. I think this guy's called Paralescus, but hey, I'm not the insect guy here. So there were two different taxonomies given: the yellow one and the green one. One was maybe, let's say, from 2005. The other one was from '95. And the expert kind of does a reassessment in the year, maybe 2015, says, "OK, let me look at what these guys have done. What these guys have done, I'm going to connect these up." Or maybe, one is the preexisting taxonomy and one is the new one that the scientist is proposing. And in order to propose this new taxonomy, it would be very helpful if you would link it to the old one, so it's clear what it is that you're actually claiming. So in this particular case, it's from a real example, so it's published in the literature. You would get this result. So there's a lot of names that the both taxonomies agree and the both vocabularies have really congruent terms. But then, there's also lots of overlap situations. These dash lines correspond to an overlap. So for example, concept number 20 here – this is just to keep the examples of easily to reference. This 20 here in taxonomy one is contained sort of a subset of what taxonomy two calls 44. So 20 is a subset of 44. But it's not as pretty as that because there's another concept here, 47, that's sort of smaller than 44, and that one overlaps with 120. So the relationships between the different nodes or different sets or different classes can be rather complex, but at least, now you know how it looks.
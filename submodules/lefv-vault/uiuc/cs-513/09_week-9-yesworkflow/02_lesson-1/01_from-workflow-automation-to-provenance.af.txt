موسيقى So we had seen before that using
provenance is important for transparency and reproducibility. If we want to explain how a certain
data product was arrived at, how we came to that. We want to explain the origin,
what are the input data, what are the processing steps,
and we want to do that. Now a lot of workflows
are created using scripts. So if you have a script, it's kind of
a not obvious what the workflow is underlying it, and looking at the code
itself is often not that helpful. So if you compare workflows with
scripts with respect to some of these properties that we've talked about before,
automation, scaling, abstraction and provenance. It appears that scripts are,
in particular, not well suited from
abstraction point of view. I mean there's executable code,
it might be nicely written, it might be nicely documented, but
it still does not provide a high-level overview of what the method is,
how it works. In particular what it does not do is it
does not explain the output products of a script, in terms of the inputs and
in terms of the computational steps. It really doesn't do that job. There is a tool called noWorkflow, I'm
going to mention it again briefly today. But even that tool, although it traces
the actual execution history of a Python script, it is not that well suited
to give you the high level overview. And also not that well suited to
explain how output depend on inputs. It will do that at the scripting level,
at the code level, but not at the more foundational level. So that was one of the goals of this
tool that I'm, again, summarizing here, called YesWorkflow. So abstraction and provenance is really
what we wanted to provide for scripts. So the big question then, is how do you
go from a script to a workflow diagram? Because this workflow diagram that you see
here on the lower right is one that would allow you precisely to explain the outputs
in terms of computational steps. Those are the green boxes,
and then input items, those would be sort of towards the top,
the notes on the top. So again, we have the green nodes or boxes
to represent steps, computational steps, and then the yellow rounded boxes
to represent data elements. And you don't have to worry
what they right now mean, but you see there's sort of a overall
data flow created that way. And it's not quite clear how
you would get it from a script, unless you yourself would declare it. So that's the YesWorkflow approach. The script author declares sort of
these dependencies, and then we have this artifact that we can share with
others and that we can also query. So how do we do that? We've seen this previously, we have code,
if we have the script already existing. And then we put annotations that mark
the beginning and end of steps, and that mark the input and output of steps. You could write this model
also next to the script. The fact that we embed it
in scripts often leads to some sort of misunderstanding
that we're documenting the code. So let me say it just one more time,
maybe for the fun of it,
write it into a separate file. Then nobody will think that's
a comment of the code, it is really a new model that you create
for your application or for your script. So I showed this example in the past,
we call it the YesWorkflow Recon. This is an example where we use the
workflow diagram on the left, to explain and reconstruct some of the provenance
that was left behind by the script. So scripts often read and
write files from various folders, and often there's underlying organization. So this particular script, for example,
takes inputs from the run folder. There's kind of raw data or
input data sub folder, it's not called input data,
it's called raw. And then there are sort of
these nested folder structures. Something representing what
is called a cassette ID. This has to do with samples that
are organized into cassettes. And then these cassettes are put
on an instrument, and sort of, images are taken with an x-ray approach. And there's different electrons
volts being used, 10,000 or 11,000. And these are sort of the raw images. But as part of the data processing
workflow, in this case the script, transformed images are generated,
sort of, they are processed. And the outputs end up in a data folder,
further down here, called data. And then you see there's
some sort of correspondence, actually these are the cassettes, Q55. But this level, Q55 is no longer
here in the processed data. Then we only care about the sample ID. So I may have spoken before,
I think Q55 is the cassette ID, and DRT240 is the sample ID. And then once we've processed it,
we kind of don't care about the cassettes. That was kind of an artifact of
the actual physical x-ray imaging. But the sample ID allows
us to really talk about the particular sample
that we're looking at. And so
these show up in the transformed images. But if you had only the script, and
if you had only these products on disk, it would be really hard to understand
how they relate to one another. If on the other hand,
you have created this graph model here, you can now understand how the outputs
are derived from the inputs. Yeah, so you see for example, here the corrected image came from the raw
image via a step called transform images. And there's, in particular,
these little expressions in here, that we call URI templates. And again, that's explained in some more
detail in the reading, how that works. But fundamentally, every expression
here that's in curly braces, you can think of it as a parameter or
a template variable. And you can then find
the corresponding values for that variable by looking on disk
in the corresponding places. So, for example, here we see
the corrected image says, data/, and then we have a variable sampleID/, another
variable called sampleID_energyLevel. So if you look in the output data,
check this out. So you have DRT240 and
then we have DRT240_10000 electron volt. So that makes that file name also
itself kind of self-contained, right? So we have not just an image number 137,
and so on, we have that all-important sample
ID right there in the file name. So if this file goes traveling somewhere,
that sample ID is still with it, if the file name isn't changed. And so we declare, in some sense,
in our conceptual model, if we want to think of that as
a conceptual model, the corrected image to be on disk in a certain file that has
these certain variables embedded. Both in the file path as well
as in the file name itself. And once we have that, we can then really
answer questions about the provenance. So for example, here is again,
the overall approach, when we say first we annotate the script using begin,
end, in, and out declarations. And we can look at the graphs. But then we can also run the script,
look what artifacts are left behind. And then use a special reconstruction step
to collect the run time provenance, and link it to perspective provenance
to the workflow graph. And then this allows us to answer
the user's provenance queries. And I'm going to show this hopefully in
a little demo in a while, to see how that actually works on a demonstration
prototype that we've put together. So again, here is the example
that I just showed you. I want to just quickly go
through this one more time. So if you want to know what samples did
the script run collect images from? The sample ID is found here, for
example, in the input folder. And this is, if you look here, raw image, we see that raw
image has this URI template. And we see that the sample ID is below
the cassette ID, so we go to run, raw, and then comes cassette ID,
and then sample ID. So run, raw, cassette ID,
and then sample ID. So in this case there were two samples,
240 and 322. And how do we know that those are samples? Because we've said so,
the script author has said so. Now if we want to know well
what energies were used, again this information is available. So the original raw image that
was first taken, had for example, in this case for 322, image number
one is was used with 10,000 volts. And then there's another image
which that was taken with 11,000. So apparently here the frame
numbers are not unique. They are only sort of unique
relative to the given sample and to the given electron volts. Okay, so
you have to be a little bit careful. Maybe you could use also
globally unique image ID. Maybe that would be
another way to model this. But again, this information has been declared by
the user as part of the YesWorkflow model. Another one, where's the raw image of
the corrected image with this file name? Again, because we understand
the structure of files names, because we've declared it, we can kind of
read this out and it becomes an artifact. And we can look up where
the raw images for those. And maybe if we see some strange artifact,
for example, in the transformed images, we can then look at the raw images to see
whether those artifacts are there as well. Maybe they were introduced as a problem
in the transform images step. Or maybe they were in the original image, which indicates our problem
is further upstream. So again, this information would be
known because we have these variables in our model. And therefore, we can query them. So this summarize what this
YesWorkflow approach gives us. So first of all,
we've talked about scientific workflows. Scientific workflows have as a goal,
many things. And I'll summarize briefly here as
automation, scaling, abstraction, and provenance. And in particular, we look at two kinds
of provenance information from workflows. One we called prospective provenance, and
that was the workflow diagram itself. The workflow itself is a form
of prospective provenance. It gives us a recipe how we can
understand a method in terms of steps and the data that flows between those steps. And in retrospective provenance,
we want to call all this information that might be available after the run or
during the run. This can include,
as we've seen, file names and folder names, but also events like
reading and writing of files. Possibly the time stamps,
although time stamps are sort of not an ideal sort of explanation
of what just happened. But they give you sometimes an indication,
if something might have gone wrong, if the time stamps don't seem to agree
with other provenance information. The YesWorkflow approach recognizes
that scripts can be workflows too. And what really the point of YesWorkflow
is to reveal the computational steps and the data dependencies of the output
data products on the inputs and on the computation steps themselves. So it is not about commenting your code. Instead, what it does is declare
the workflow model in a way that explains the relevant data dependencies,
outputs in terms of the inputs. موسيقى
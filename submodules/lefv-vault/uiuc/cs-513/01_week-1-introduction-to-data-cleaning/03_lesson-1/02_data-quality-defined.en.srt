1
00:00:07,169 --> 00:00:10,939
Welcome back to theory and practice of data cleaning.

2
00:00:10,939 --> 00:00:13,144
In this video, we will ask the question,

3
00:00:13,144 --> 00:00:16,000
what is data quality and how we can improve it?

4
00:00:16,000 --> 00:00:19,426
So first, let's look again at data quality and data cleaning in context.

5
00:00:19,426 --> 00:00:22,734
Data cleaning or data wrangling is a much needed but

6
00:00:22,734 --> 00:00:26,320
often under-appreciated phase before data analysis can begin.

7
00:00:26,320 --> 00:00:27,370
As we've seen last time,

8
00:00:27,370 --> 00:00:31,329
low quality data causes significant costs and that doesn't matter whether we clean

9
00:00:31,329 --> 00:00:32,750
the data and just live with

10
00:00:32,750 --> 00:00:36,435
the dirty data or whether we spent the effort on cleaning the data.

11
00:00:36,435 --> 00:00:41,079
In both cases, it is the low quality data that causes the significant costs.

12
00:00:41,079 --> 00:00:44,649
And obviously, we want to apply data cleaning if we have to do it

13
00:00:44,649 --> 00:00:48,880
early in the data lifecycle and apply again if we have to,

14
00:00:48,880 --> 00:00:53,329
or preferably, we try to create the data without data quality problems.

15
00:00:53,329 --> 00:00:54,579
But in the real world,

16
00:00:54,579 --> 00:00:59,189
we often have these issues and we need to look at the data errors that are in our data.

17
00:00:59,189 --> 00:01:01,500
And also last time we had a brief look at

18
00:01:01,500 --> 00:01:05,030
the different types of data errors that a dataset might have.

19
00:01:05,030 --> 00:01:09,449
So, we mentioned briefly quantitative errors or outliers,

20
00:01:09,449 --> 00:01:12,540
which are focused typically of classes

21
00:01:12,540 --> 00:01:17,521
around statistics and the focus in this class qualitative errors,

22
00:01:17,521 --> 00:01:20,319
which are syntax or formatting errors, for example,

23
00:01:20,319 --> 00:01:24,554
pattern violations and semantic or schema errors such as integrity constraints.

24
00:01:24,554 --> 00:01:26,820
Those two kinds will be the focus of this course.

25
00:01:26,820 --> 00:01:29,915
Now, a recurring topic here is data quality.

26
00:01:29,915 --> 00:01:33,700
And data quality, just like quality in general,

27
00:01:33,700 --> 00:01:37,114
is not easily defined as this author Robert Pirsig

28
00:01:37,114 --> 00:01:41,155
of a famous novel Zen and The Art of Motorcycle Maintenance says,

29
00:01:41,155 --> 00:01:44,500
"Even though quality cannot be defined, you know what it is."

30
00:01:44,500 --> 00:01:46,659
So once we see it, maybe we know what it is.

31
00:01:46,659 --> 00:01:48,655
We will study some examples as well.

32
00:01:48,655 --> 00:01:51,400
However, we want to be a little bit more proactive and

33
00:01:51,400 --> 00:01:54,390
we'll look at what data quality is in our context.

34
00:01:54,390 --> 00:01:57,099
So first of all, let's make an attempt.

35
00:01:57,099 --> 00:01:59,734
And this is a common definition of data quality.

36
00:01:59,734 --> 00:02:02,049
Data quality is often defined as fitness for

37
00:02:02,049 --> 00:02:05,515
use or fitness for purpose. What does that mean?

38
00:02:05,515 --> 00:02:09,669
We can say that data are of high quality if they are fit for use in

39
00:02:09,669 --> 00:02:14,379
their uses (by the customers) in operations, decision-making and planning.

40
00:02:14,379 --> 00:02:17,050
Another way of saying this or looking at this is to say,

41
00:02:17,050 --> 00:02:19,585
they are fit for use when they are free of defects

42
00:02:19,585 --> 00:02:22,629
and possess the features needed to complete the operation,

43
00:02:22,629 --> 00:02:24,655
make the decision or complete the plan.

44
00:02:24,655 --> 00:02:27,789
Or in our case, in data science and data analytics,

45
00:02:27,789 --> 00:02:32,294
if they are good enough and free of defects so we can do the data analysis that we want.

46
00:02:32,294 --> 00:02:35,960
Again, as usual, I have some references here on the slides.

47
00:02:35,960 --> 00:02:40,870
So, this notion of data quality and fitness for use is basically a good one.

48
00:02:40,870 --> 00:02:43,990
It comes from the idea that we first have to ask the question,

49
00:02:43,990 --> 00:02:45,699
what is it that we want to do with the data?

50
00:02:45,699 --> 00:02:51,050
Specifically, what are the questions we're trying to answer from the data using the data?

51
00:02:51,050 --> 00:02:52,300
And in this context as we know,

52
00:02:52,300 --> 00:02:55,930
that data cleaning of dirty data can be very expensive for creating

53
00:02:55,930 --> 00:02:59,800
high quality data in the first place can be very expensive.

54
00:02:59,800 --> 00:03:03,270
The question is often, do we even need this particular data set?

55
00:03:03,270 --> 00:03:07,925
Do we even need this particular table or column or field in this dataset?

56
00:03:07,925 --> 00:03:10,044
And so this is why we ask,

57
00:03:10,044 --> 00:03:13,569
is the data maybe already good enough for our use?

58
00:03:13,569 --> 00:03:15,100
Can we already answer that question?

59
00:03:15,100 --> 00:03:16,539
Or is it hopeless, maybe?

60
00:03:16,539 --> 00:03:18,895
There's no way that given certain data set,

61
00:03:18,895 --> 00:03:20,500
we can answer the questions that we want.

62
00:03:20,500 --> 00:03:22,884
So take for example,

63
00:03:22,884 --> 00:03:25,884
you might have a census data set available,

64
00:03:25,884 --> 00:03:32,365
you're doing some citizen science project or some data journalism project,

65
00:03:32,365 --> 00:03:35,884
and you want to analyze the census data say by region or

66
00:03:35,884 --> 00:03:39,381
geographic region or by state or by county.

67
00:03:39,381 --> 00:03:42,875
So, some depending on the analysis questions,

68
00:03:42,875 --> 00:03:45,289
you may or may not need to know what is

69
00:03:45,289 --> 00:03:49,580
the population of a particular area or region or a county.

70
00:03:49,580 --> 00:03:53,539
Or if your analysis does not require information about the population,

71
00:03:53,539 --> 00:03:57,814
the accuracy and data quality in that column might not be important.

72
00:03:57,814 --> 00:04:01,740
Similar for place names, latitude, longitude.

73
00:04:01,740 --> 00:04:03,860
So there's a lot of different types of data,

74
00:04:03,860 --> 00:04:05,800
different columns in such a data set

75
00:04:05,800 --> 00:04:08,685
and clearly depending on the question that you're asking,

76
00:04:08,685 --> 00:04:17,495
you may or may not bother to clean up or check the quality of data in certain columns.

77
00:04:17,495 --> 00:04:21,569
So, this is where the notion of fitness for use is very helpful.

78
00:04:21,569 --> 00:04:24,410
So, but there are cases where this gets tricky.

79
00:04:24,410 --> 00:04:27,935
So for example, if you don't yet know what you want to ask of the data,

80
00:04:27,935 --> 00:04:30,204
maybe you create data for the first time,

81
00:04:30,204 --> 00:04:32,420
the question is, what should I capture?

82
00:04:32,420 --> 00:04:36,829
There are so-called minimal information standards that sometimes try to capture

83
00:04:36,829 --> 00:04:41,579
the information that you definitely need no matter what the use of the data subsequently.

84
00:04:41,579 --> 00:04:44,750
Also, this is challenging the notion of fitness for use,

85
00:04:44,750 --> 00:04:46,670
challenging when you are for example,

86
00:04:46,670 --> 00:04:49,610
an archivist- a digital archivist, maybe,

87
00:04:49,610 --> 00:04:53,930
a digital librarian or somebody who works for a research data library.

88
00:04:53,930 --> 00:04:58,295
Their job is to archive the data as it comes to the repository to the archive.

89
00:04:58,295 --> 00:05:00,560
But they cannot assume or know

90
00:05:00,560 --> 00:05:04,055
anything about the future use of the data, not necessarily.

91
00:05:04,055 --> 00:05:08,444
So how would we know whether there's any operations that we need to do?

92
00:05:08,444 --> 00:05:11,990
So typically, as an archivist you're not trying to improve the data,

93
00:05:11,990 --> 00:05:14,029
that would be in fact,

94
00:05:14,029 --> 00:05:17,120
a misuse of your role as an archivist.

95
00:05:17,120 --> 00:05:21,009
You're supposed to archive the data as it comes.

96
00:05:21,009 --> 00:05:22,780
On the other hand, if you are a data curator,

97
00:05:22,780 --> 00:05:28,189
say maybe you work in a Natural History Museum and you're creating a data set

98
00:05:28,189 --> 00:05:30,740
there then you might want to improve

99
00:05:30,740 --> 00:05:34,154
the data quality because if you have bad quality data,

100
00:05:34,154 --> 00:05:38,720
maybe latitude, longitude of specimen data is incorrect or

101
00:05:38,720 --> 00:05:43,639
the species names are not standard or dates cannot be recognized and so on.

102
00:05:43,639 --> 00:05:49,120
Then the data set that you're maybe maintaining is quite useless for subsequent uses.

103
00:05:49,120 --> 00:05:51,845
So in that case, you need to in fact,

104
00:05:51,845 --> 00:05:56,264
think about the future use and then possibly need to improve the data.

105
00:05:56,264 --> 00:05:58,220
So looking at the broad picture,

106
00:05:58,220 --> 00:06:02,149
here are three pillars of data quality that have

107
00:06:02,149 --> 00:06:06,296
been identified by a chapter in the Handbook of Data Quality.

108
00:06:06,296 --> 00:06:07,850
They are the organizational,

109
00:06:07,850 --> 00:06:10,605
architectural and computational pillars.

110
00:06:10,605 --> 00:06:11,944
So by organizational pillar,

111
00:06:11,944 --> 00:06:14,980
we mean the data quality objectives for the organization.

112
00:06:14,980 --> 00:06:17,420
The strategy is to establish roles, processes,

113
00:06:17,420 --> 00:06:22,074
policies and standards required to manage and ensure data quality objective.

114
00:06:22,074 --> 00:06:24,245
So again, there is implicitly this notion

115
00:06:24,245 --> 00:06:27,139
of fitness for purpose and data quality as well.

116
00:06:27,139 --> 00:06:28,339
What is our objective?

117
00:06:28,339 --> 00:06:30,149
What is it that we want to do with the data?

118
00:06:30,149 --> 00:06:33,860
Now, the architectural pillar refers to the technology landscape to

119
00:06:33,860 --> 00:06:37,944
deploy a data quality management process or standard or policy.

120
00:06:37,944 --> 00:06:40,985
And the focus of this class is really on the computational side,

121
00:06:40,985 --> 00:06:42,290
where we look at I.T.

122
00:06:42,290 --> 00:06:47,589
tools and computational techniques required to meet data quality objectives.

123
00:06:47,589 --> 00:06:53,634
So concrete examples of these are syntax and format normalization, integrity constraints,

124
00:06:53,634 --> 00:06:56,779
notions such as provenance or the data lineage and processing

125
00:06:56,779 --> 00:07:00,254
history of data and there are many further computational topics,

126
00:07:00,254 --> 00:07:02,430
not all of which we are covering in this course.

127
00:07:02,430 --> 00:07:04,600
For example, duplicate detection.

128
00:07:04,600 --> 00:07:08,774
All right, so what are the common phases and steps in data quality management?

129
00:07:08,774 --> 00:07:12,170
So occasionally, we have to actually understand the context

130
00:07:12,170 --> 00:07:15,839
of the information or organizational processes.

131
00:07:15,839 --> 00:07:18,751
And this is called the context reconstruction.

132
00:07:18,751 --> 00:07:21,170
But this phase can sometimes be skipped if

133
00:07:21,170 --> 00:07:23,329
this context information and background information is

134
00:07:23,329 --> 00:07:25,670
already available from previous analyses.

135
00:07:25,670 --> 00:07:28,498
But the next two phases are really critical,

136
00:07:28,498 --> 00:07:31,420
pretty much to all data quality management processes.

137
00:07:31,420 --> 00:07:34,058
First one then is assessment or measurement,

138
00:07:34,058 --> 00:07:37,189
so the idea here is that we measure data quality along

139
00:07:37,189 --> 00:07:41,389
relevant dimensions and we assess the data quality by comparing it often,

140
00:07:41,389 --> 00:07:43,355
for example, with reference values.

141
00:07:43,355 --> 00:07:46,810
And this then allows us to do some diagnosis of the data quality.

142
00:07:46,810 --> 00:07:49,399
And often, we can then also find some causes of

143
00:07:49,399 --> 00:07:51,875
poor data quality and possibly rule those out

144
00:07:51,875 --> 00:07:57,055
in the future or automate the removal of such low quality data.

145
00:07:57,055 --> 00:07:59,774
So assessment and measurement is critical.

146
00:07:59,774 --> 00:08:02,750
And then, of course, we come to the data cleaning,

147
00:08:02,750 --> 00:08:05,045
proper phase, the improvement.

148
00:08:05,045 --> 00:08:10,310
So here we are, interested in the selection of the steps and strategies and techniques.

149
00:08:10,310 --> 00:08:13,959
Also, the use of tools for reaching the data quality targets.

150
00:08:13,959 --> 00:08:18,235
Dimensions of data quality is rather a vast topic.

151
00:08:18,235 --> 00:08:21,500
A lot of research has been conducted to

152
00:08:21,500 --> 00:08:25,589
look at data quality dimensions in quite some depth,

153
00:08:25,589 --> 00:08:28,310
and there are very many dimensions,

154
00:08:28,310 --> 00:08:30,182
over 100 or so.

155
00:08:30,182 --> 00:08:33,455
However, as you can find from this reading,

156
00:08:33,455 --> 00:08:35,105
that is mentioned here,

157
00:08:35,105 --> 00:08:38,539
there are a couple of dimensions that

158
00:08:38,539 --> 00:08:41,990
have to do with data values that show up pretty much everywhere.

159
00:08:41,990 --> 00:08:43,301
And these data quality dimensions,

160
00:08:43,301 --> 00:08:45,125
we'll have to take a closer look.

161
00:08:45,125 --> 00:08:48,034
They are accuracy, completeness,

162
00:08:48,034 --> 00:08:51,279
consistency and timeliness of data.

163
00:08:51,279 --> 00:08:53,165
So, what do we mean by accuracy?

164
00:08:53,165 --> 00:08:56,225
Accuracy, we mean the extent to which data are correct,

165
00:08:56,225 --> 00:09:01,634
reliable and corresponds to the ground truth for the real world values.

166
00:09:01,634 --> 00:09:03,289
And often, in practice,

167
00:09:03,289 --> 00:09:05,585
we focus on syntax and patterns.

168
00:09:05,585 --> 00:09:11,366
For example, very concretely you might use regular expressions to match dates,

169
00:09:11,366 --> 00:09:14,360
this is one of our first exercises.

170
00:09:14,360 --> 00:09:16,399
So, is it even possible that

171
00:09:16,399 --> 00:09:20,845
a certain date is even valid and represents a real world date?

172
00:09:20,845 --> 00:09:25,480
Or can we parse a date field in a dataset and it

173
00:09:25,480 --> 00:09:31,059
corresponds to another or to all the other date fields in the dataset?

174
00:09:31,059 --> 00:09:33,585
Completeness, refers to the degree to which a data set

175
00:09:33,585 --> 00:09:36,789
includes necessary information about relevant objects.

176
00:09:36,789 --> 00:09:38,580
If a data set is very incomplete,

177
00:09:38,580 --> 00:09:44,029
clearly certain uses we cannot allow or will not work.

178
00:09:44,029 --> 00:09:48,580
So, sometimes, we have to have mostly complete data or very complete data.

179
00:09:48,580 --> 00:09:50,029
What is consistency?

180
00:09:50,029 --> 00:09:54,065
Again, here we have actually tools that can check consistency

181
00:09:54,065 --> 00:09:58,955
by checking whether integrity constraints are satisfied or violated.

182
00:09:58,955 --> 00:10:01,960
So, these are constraints possibly semantic

183
00:10:01,960 --> 00:10:04,879
constraints or schema level constrains that we

184
00:10:04,879 --> 00:10:11,879
know should hold for a given application for a given dataset or a set of tables, maybe.

185
00:10:11,879 --> 00:10:14,870
And so, we look quite closely in the use

186
00:10:14,870 --> 00:10:19,320
of database technology to check the consistency of data.

187
00:10:19,320 --> 00:10:21,215
And finally, the notion of timeliness,

188
00:10:21,215 --> 00:10:23,254
sometimes also called currency,

189
00:10:23,254 --> 00:10:25,664
and the related notion of volatility.

190
00:10:25,664 --> 00:10:27,634
So, since they change over time,

191
00:10:27,634 --> 00:10:29,684
working with the data set might be problematic.

192
00:10:29,684 --> 00:10:31,835
And so we have to answer certain questions,

193
00:10:31,835 --> 00:10:36,769
if we want to know whether the data is of good enough quality with respect to timeliness.

194
00:10:36,769 --> 00:10:39,169
So for example, what is the delay between

195
00:10:39,169 --> 00:10:42,560
the change in the real world and the change in the database?

196
00:10:42,560 --> 00:10:44,029
If our data is very old,

197
00:10:44,029 --> 00:10:46,465
it might be stale, it might be low quality,

198
00:10:46,465 --> 00:10:48,125
with respect to timeliness,

199
00:10:48,125 --> 00:10:51,789
it might not work for our analysis questions at hand.

200
00:10:51,789 --> 00:10:55,399
Another way to ask this is how long is the data a valid in the real world?

201
00:10:55,399 --> 00:11:00,009
And more generally, is the data still appropriate for the purpose at hand?

202
00:11:00,009 --> 00:11:05,990
To summarize, we have defined data quality as fitness for use.

203
00:11:05,990 --> 00:11:09,950
We have learned about three pillars of data quality: organizational,

204
00:11:09,950 --> 00:11:15,090
architectural and computational with the latter being the focus of this course.

205
00:11:15,090 --> 00:11:16,554
And we've also learned about

206
00:11:16,554 --> 00:11:21,080
the quality management phases on text reconstruction and then,

207
00:11:21,080 --> 00:11:23,659
the important assessment or measurement of

208
00:11:23,659 --> 00:11:26,588
data quality and then improvement of data quality,

209
00:11:26,588 --> 00:11:28,846
really the key focus of this course.

210
00:11:28,846 --> 00:11:29,929
And last, not least,

211
00:11:29,929 --> 00:11:33,029
we have looked at data quality dimensions in particular,

212
00:11:33,029 --> 00:11:34,924
the dimentions of accuracy,

213
00:11:34,924 --> 00:11:38,639
completeness, consistency and timeliness
Welcome to theory and practice of data cleaning. In this video, I will give you an overview and introduction of the course. Let's first have a look at data wrangling versus data analytics. Data wrangling refers to data processing that allows meaningful analysis to begin subsequently. It includes steps, for example, data extraction, transformation, and loading, also known as ETL, but also data integration, data cleaning, querying, and repairing databases. Unfortunately, database people are often not very good at PR, so most of the glamorous data analytics gets all the attention and the equally important and often much more time consuming task of data wrangling and data cleaning. If you look at what skills data scientists should have, we can look for example, as a study here, from CrowdFlower.com where the company has extracted LinkedIn profiles relating to data science job openings. And you will notice that database skills are rather dominantly required here. So for example, you see that SQL is the most in-demand skill from this particular study, but also there are indirectly other database skills required, for example, for NoSQL databases, but also for Oracle, MySQL, Postgres and so on. Now, in this course, the focus is not on teaching you database skills although it will be helpful if you have such skills already. On the other hand, we will employ ideas from databases and, in particular, would like to teach you how to think like a database person because this will become very handy in our tasks. Now first let's take a look at what the costs are that result from low-quality data. Throughout this course, I will give you examples of readings that you can read accompanying the course material. And here's an example of such a reading from the "Handbook of Data Quality." It's called "Cost and Value Management for Data Quality." This chapter reports on many of the costs that can occur as a result of low data quality. And often these costs are reported to be in the billions of dollars, but they can also occur sort of in more modest forms for the long-tail of science, that is, where scientists who were just doing their lonely job of data analysis often find it very difficult to create data or collect data of high quality. Also in data journalism for example, when people want to collect open-source data and then analyze it and write about it. So, low-quality data is a issue in big industry but also in academia. So, it's a big data science issue. From the same article, you can also have an overview of what kind of costs exists due to data quality issues. For example, in the one hand, you can have the costs that result from low data quality, so the effects of low data quality. So there are direct and indirect costs. On the other hand, you can try to, of course, look at what happens to improve the quality of data. What are the costs there, for maybe, preventing the dirty or low-quality data in the first place or detecting the data and then, of course, for repairing it. Again, I've given you here an example reading to look further into these issues. But to get a feeling of what these issues are, to get more specific, let me just enumerate a few of them. So for example there are data maintenance costs or personal costs, data search costs. All of these costs can be increased due to low-quality data. Semantic confusion cost, if you're looking for the wrong data, if you have to re-enter data, if you misinterpret data, you might lose revenue if in industry or you might lose current or future customers, or even orders. So there's a very large number of effects that result from low-quality data. Now let's turn to the big idea of data cleaning and what we're doing in this course. So the overall idea is to, what we're aiming to understand, to assess, and then improve data quality. So you can remember these issues with the following three Qs. We have quality dimensions. That means data should be of high quality according to a number of dimensions; it should be accurate; it should be timely; it should be relevant, complete and so on. But then, data quality, how is it defined in the first place? We can think of it, a state of high quality, if it allows us to answer the questions that we want to ask. So this term is also sometimes used, it's called fitness for use or fitness for purpose. Is the quality of the data sufficient to answer the questions that we have? And then finally, we turn the questions often into queries, and this is where we take a database person's perspective. So for example, we want to execute database queries in order to get an overview of what's kind of data we have, that's called data profiling. Or, more specifically related to data cleaning, we might want to check the integrity constraints. So, certain quality metrics can be translated into integrity constraints, and we can then check whether data satisfies these integrity constraints and, if necessary, deal with the outliers, deal with exceptions and so on, and repair the data. So let's also look at data cleaning in context. Where does it fit in the larger data lifecycle? So, initially, data comes from various sources, and so we have to gather and select that data that we want to work with. We then have to profile the data. We want to identify and then quantify that data quality issues that the data might have. And then, depending on the error types, we can deal with the causes and try to repair the data. In the data cleaning phase, the proper data cleaning phase then, one of the main task we often do is we standardize the data, we bring it into a common format, we normalize it, and we might even be using controlled vocabularies so that the values that we find in the data conform to these standard vocabularies. And then finally, we have the data analysis phase proper. This is, of course, covered in other courses when we do data analysis methods. This is not always such a linear pipeline, the data lifecycle, but rather we can even use sometimes methods from data analysis for data cleaning, so it is often a cycle. I want to also mention that data integration and data warehousing are issues where data quality is particularly important because we combine information from various sources. And because of the various sources where we integrate the data from, we, in particular, find data quality issues often that we need to deal with. So here is a simple taxonomy of error types. This is taken from one of the readings associated with this course. It's called "Detecting Data Errors: Where Are We and What Needs to Be Done?". So at the very highest level, we can think of quantitative errors versus qualitative errors. For quantitative errors, we think of them as outliers, for example. That means if certain values of the data deviate significantly from the distribution of the values. To detect outliers, we typically apply methods from statistics, data mining, and machine learning, and these are outside the scope of this course. On the other hand, qualitative errors often deal with syntactic variations, maybe of different ways to spell a place, a location, different formats to spell dates for example, and these syntactic violations we need to deal with and this is part of this course. Similarly, at a higher level, we have then also schema issues and integrity constraint violations. So for example, people who know databases will know there are integrity constraints such as functional dependencies, foreign key dependencies or inclusion dependencies. These allow us to check the quality of the data with respect to certain constraints that come from the schema or from the semantics of an application. And so syntactic and schema level issues are the focus of this course. There are also other issues that can be considered qualitative errors, for example, detecting duplicates and removing duplicates. So let's take a closer look at the course themes, topics, and in even the tools that we're going to study in this course. So among the syntactic issues, we will begin with studying regular expressions, those defined patterns that can be used to match, extract, and transform data. That is, we will deal with syntactic variations in the data. A particular tool that we're going to use, and it's rather powerful, it's called OpenRefine. This is an open source tool, one of the few open-source tools for data wrangling. In order to deal with schema and semantic issues, we'll wear a database person's hat. We're gonna use database technologies for data profiling, for querying data, but also for checking integrity constraints, and even we are venturing a little bit into database repair. The languages and tools we're gonna use are Datalog and SQL. And finally, in the last block of topics and themes here, in this course, we look at synthesis issues. That means when we put together a data analysis pipeline and the data cleaning pipeline, for example, using scripts. In this context, it will be important to get an understanding of what is the workflow that puts everything together. So we will look at workflow automation and in this context in particular it's important to get also an understanding of the provenance of data. Provenance describes the lineage and history, processing history, of data, and this is something we will study towards the end of this course. And the tool we're gonna use there is a prototypical tool that's developed as part of a research project, and it's called YesWorkflow. It allows to put to to model scripts that automate data cleaning or data analysis tasks as workflows and then work with this workflow, for example, to understand data dependencies or to query provenance information. So let's look take a quick look at some of these tools. So here's the OpenRefine tool. OpenRefine was formerly called Google Refine, and it's now an open-source tool. So we will explore data using this tool. It has a feel quite similar to a spreadsheet, so when you load the data, it looks like you're working with a spreadsheet application, however there's particular functionality for data cleaning built into OpenRefine. Once we have put into OpenRefine and cleaned it, we can load it into a database. Once in a database, however, not all the data issues have gone away, and so instead we need to work with database technology to identify and possibly repair data problems. So here on this screen, you'll see two tables: a person table and an address table. And you will see a number of data quality issues here. So let's quickly go through them. So, for example, you see that the identifier of a person should probably be unique, but on the left, you see that 43 is used as an identifier for two different rows. So that's clearly a data quality problem. Also, the format of names seems to be not unique, so we have, for example, last name comma first name or the first name followed by last name without a comma. The format of the date of birth column -- that's the third column in the person table -- is vary is varying. There's very different formats. So this will be very problematic when we want to query this data or analyze this data. So, ideally, this should have been taken care of before we load the data into a database. So there are numerous issues here with this data due to different representations and formats. There are literally contradictions in the data, so you can find here, analyzing the tables, you can try and find these contradictions. There are incorrect values, typos. There's duplicates, and there are also integrity constraints violated that, for example, that database system normally would detect or enforce. Last not least, we might also have issues where the data is incomplete. So this is indicated in the phone column, for example, there are two rows where the phone numbers apparently are not given. One is with a special value, lots of nines there, and the other is an explicit null value. All of these issues are data quality issues that we might encounter even after having used the tools such as OpenRefine, and we will study database means to deal with these issues. So in the last part of this course, we will cover workflows and provenance. This is one of the aspects mentioned before the synthesis, when we have workflow automation to automate data cleaning or data analysis pipelines. In practice, often scripts are used, and we will study a particular tool called YesWorkflow that allows to put annotations inside of scripts, thereby revealing and recreating a workflow. We can then use this workflow diagram and query it and understand data dependencies, for example, and the data lineage and provenance of data coming out of such workflows or scripts.
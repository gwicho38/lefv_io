1
00:00:00,000 --> 00:00:08,591
[MUSIC]

2
00:00:08,591 --> 00:00:11,060
Welcome to theory and
practice of data cleaning.

3
00:00:11,060 --> 00:00:11,630
In this video,

4
00:00:11,630 --> 00:00:15,790
I will give you an overview of the topics
we will be covering in this course.

5
00:00:15,790 --> 00:00:19,710
As the course title suggests, we will be
looking at both, theoretical foundations

6
00:00:19,710 --> 00:00:23,320
and concepts of data cleaning, but
also learn very practical issues.

7
00:00:23,320 --> 00:00:24,238
In particular,

8
00:00:24,238 --> 00:00:28,442
you will learn practical skills to
clean data with freely available tools.

9
00:00:28,442 --> 00:00:30,940
But first, why do we need to clean data?

10
00:00:30,940 --> 00:00:32,430
What is dirty data, anyways?

11
00:00:33,460 --> 00:00:36,260
Let us use a simple example
to illustrate the problem.

12
00:00:36,260 --> 00:00:39,070
Assume you have identified a data
set that you need to analyze for

13
00:00:39,070 --> 00:00:40,620
a data science project.

14
00:00:40,620 --> 00:00:43,770
Maybe you have already extracted
the data from various sources and

15
00:00:43,770 --> 00:00:45,110
brought it into a tabular format.

16
00:00:46,120 --> 00:00:47,900
As you look through the rows
of your data tables,

17
00:00:47,900 --> 00:00:53,000
you notice that important columns show
multiple non-unique representations.

18
00:00:53,000 --> 00:00:55,480
For example, for dates or locations.

19
00:00:55,480 --> 00:00:56,310
Let's take the simplest,

20
00:00:56,310 --> 00:01:00,610
yet very widespread issue, different
representations of calendar dates.

21
00:01:00,610 --> 00:01:04,000
This comic from xkcd nicely
illustrates the point.

22
00:01:04,000 --> 00:01:07,970
It starts with the following,
public service announcement.

23
00:01:07,970 --> 00:01:11,920
Our different ways of writing dates as
numbers can lead to online confusion.

24
00:01:11,920 --> 00:01:15,510
That's why in 1988, ISO,
the International Organization for

25
00:01:15,510 --> 00:01:19,320
Standardization, send a global
standard numeric date format.

26
00:01:19,320 --> 00:01:21,682
This is the correct way
to write numeric dates.

27
00:01:21,682 --> 00:01:25,915
2013-02-27.

28
00:01:25,915 --> 00:01:28,736
The following formats
are therefore discouraged.

29
00:01:28,736 --> 00:01:33,860
Here the comic lists among other quite
confusing but also entertaining variance.

30
00:01:33,860 --> 00:01:38,460
The popular US convention of listing the
month, then the day, and then the year.

31
00:01:38,460 --> 00:01:40,180
Why is this a problem?

32
00:01:40,180 --> 00:01:42,598
Take any data analysis
that you want to do, and

33
00:01:42,598 --> 00:01:44,713
that means to aggregate values by date.

34
00:01:44,713 --> 00:01:46,170
This is very common.

35
00:01:46,170 --> 00:01:48,590
If you have different
representations of dates,

36
00:01:48,590 --> 00:01:51,510
your analyzed results will be incorrect.

37
00:01:51,510 --> 00:01:56,045
This is what the saying, trash in,
trash out means in this context.

38
00:01:56,045 --> 00:01:57,393
So what can we do?

39
00:01:57,393 --> 00:02:01,830
Before we can do meaningful data analysis,
we need to do some data cleaning or

40
00:02:01,830 --> 00:02:04,307
data wrangling as it is sometimes called.

41
00:02:04,307 --> 00:02:07,844
Data wrangling includes steps
such as data extraction,

42
00:02:07,844 --> 00:02:11,241
data integration data cleaning proper and
querying.

43
00:02:11,241 --> 00:02:14,612
This is something that database experts
have been working on a longtime.

44
00:02:16,201 --> 00:02:19,080
A proper databases,
it turns out that these database topics,

45
00:02:19,080 --> 00:02:22,938
that is data wrangling and data cleaning,
are usually the most time intensive parts

46
00:02:22,938 --> 00:02:26,710
of data science projects, yet they often
don't get the attention they deserve.

47
00:02:27,890 --> 00:02:31,230
It seems that here an alternative
80/20 rule applies.

48
00:02:31,230 --> 00:02:35,607
The shiny downstream data analysis
is maybe only 20% of the work, but

49
00:02:35,607 --> 00:02:37,196
its gets 80% of the PR.

50
00:02:37,196 --> 00:02:41,752
In contrast, the all important data
wrangling and data cleaning work only gets

51
00:02:41,752 --> 00:02:45,770
20% of the PR, although it
often at least 80% of the work.

52
00:02:45,770 --> 00:02:48,690
We can see that in the early
stages data is gathered, and

53
00:02:48,690 --> 00:02:51,100
data quality problems
need to be identified.

54
00:02:51,100 --> 00:02:53,950
For example, via form of data profiling.

55
00:02:53,950 --> 00:02:57,040
Next, we need to do some data
cleaning proper, for example,

56
00:02:57,040 --> 00:03:00,140
in the case of the various date formats,
we need to standardize and

57
00:03:00,140 --> 00:03:02,960
normalize the data to make it fit for
our purpose.

58
00:03:04,040 --> 00:03:06,110
So what do we mean by data quality here?

59
00:03:06,110 --> 00:03:09,920
It turns out that this is not that easy
to define, because someone's sturdy data,

60
00:03:09,920 --> 00:03:14,010
or noise, may be somebody else's signal
that they study and need to analyze.

61
00:03:15,270 --> 00:03:17,900
This is why the notion of fitness for
use or fitness for

62
00:03:17,900 --> 00:03:22,320
purpose is crucial as a guide, so
we can decide what needs to be cleaned and

63
00:03:22,320 --> 00:03:26,690
when we can consider data are good or
clean enough for the intended purposes.

64
00:03:26,690 --> 00:03:28,340
Now, let me give you
an overview of the topics and

65
00:03:28,340 --> 00:03:30,070
tools we are going to
cover in this course.

66
00:03:31,160 --> 00:03:35,165
We will start up with a very practical
hands on tools called OpenRefine.

67
00:03:35,165 --> 00:03:37,720
OpenRefine was called
Google Refine earlier and

68
00:03:37,720 --> 00:03:41,360
is an excellent interactive tool
to profile and normalize data.

69
00:03:41,360 --> 00:03:43,340
These do some key deduct
leaning functions.

70
00:03:44,342 --> 00:03:47,620
Within OpenRefine, we will sometimes
need to use regular expressions

71
00:03:47,620 --> 00:03:51,300
to specify the kinds of data we
want to match and transform.

72
00:03:51,300 --> 00:03:54,832
So we will also be learning
about regular expressions soon.

73
00:03:54,832 --> 00:03:57,825
After we're done with regular
expressions and OpenRefine,

74
00:03:57,825 --> 00:04:00,770
we can then take a data set and
load it into a database system.

75
00:04:00,770 --> 00:04:03,662
This allows us to check
the integrity of our data set

76
00:04:03,662 --> 00:04:08,299
using a powerful query language for
such tabular data, that is SQL.

77
00:04:08,299 --> 00:04:11,827
We'll also learn about a rule based
language called Datalog that allow us to

78
00:04:11,827 --> 00:04:14,930
express and
test logic integrity constraints.

79
00:04:14,930 --> 00:04:18,550
Using SQL and Datalog, we will then see
how we can check the consistency of our

80
00:04:18,550 --> 00:04:22,910
data set, and identify problematic
data that needs further attention.

81
00:04:22,910 --> 00:04:26,470
Here you see an example of the kinds of
problems that a data set might still have,

82
00:04:26,470 --> 00:04:30,320
after it has been partially
cleaned using OpenRefine.

83
00:04:30,320 --> 00:04:34,230
Some of the issues you see here could be
detected and repaired with OpenRefine,

84
00:04:34,230 --> 00:04:36,540
but others are clearly better
handled in the database system.

85
00:04:37,650 --> 00:04:40,234
For example,
the issues with date of birth or

86
00:04:40,234 --> 00:04:43,840
the DOB column is something we can try and
fix with OpenRefine.

87
00:04:43,840 --> 00:04:45,808
But what about other
integrity constraints?

88
00:04:45,808 --> 00:04:46,576
For example,

89
00:04:46,576 --> 00:04:51,710
that every ZIP code in the person table
should also show up in the address table.

90
00:04:51,710 --> 00:04:55,320
This can be easily checked with database
technology which is why we will spend some

91
00:04:55,320 --> 00:04:58,425
time on learning about integrity
constraint checking in databases.

92
00:04:58,425 --> 00:05:00,215
After these very hands-on topics,

93
00:05:00,215 --> 00:05:04,256
we will also take a brief look at cutting
edge research approaches to automatically

94
00:05:04,256 --> 00:05:08,023
repair databases based on integrity
constraints that need to be satisfied.

95
00:05:09,393 --> 00:05:12,857
In the final part of the course, we will
address two related topics, that is,

96
00:05:12,857 --> 00:05:15,320
workflow automation and prominence.

97
00:05:15,320 --> 00:05:18,850
This is another topic where we'll
get our hands dirty again, that is,

98
00:05:18,850 --> 00:05:22,110
we will write some Python scripts
to do simple data cleaning.

99
00:05:22,110 --> 00:05:26,080
The focus will then be on,
how we can model such scripts as workflows

100
00:05:26,080 --> 00:05:31,040
that document how data was processed as
part of the overall data cleaning phase.

101
00:05:31,040 --> 00:05:34,220
This is important to ensure we
produce ability of the old world and

102
00:05:34,220 --> 00:05:35,100
data science project.

103
00:05:36,270 --> 00:05:39,920
Here, we will also learn more about
the important notion of data providence,

104
00:05:39,920 --> 00:05:41,360
which describes the origin and

105
00:05:41,360 --> 00:05:44,594
processing history of data as it is
being processed in the workflow.

106
00:05:50,479 --> 00:06:00,597
[MUSIC]

107
00:06:00,597 --> 00:06:04,729
[SOUND]